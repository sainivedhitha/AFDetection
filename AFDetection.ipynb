{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atrial Fibrillation Detection\n",
    "\n",
    "This project aims to detect periods of AF from preprocessed ECG data by detecting irregular R-R intervals in the ECG signals. Different Machine Learning Classifiers are used to build an ML model using the given data. The performance of these various classifiers are analysed to determine the best classifier for this data. This is an interactive notebook that displays the results of our work. The entire source code can be found at the [GitHub](https://github.com/rushvanth/AFDetection) repository. \n",
    "\n",
    "It is highly recommended to run this Notebook in [Colab](https://colab.research.google.com/) in a GPU backed runtime. This drastically reduces run times while training models. In some cases, training time can be reduced from >5 hours (CPU processing - i7 1135G7) to ~4 minutes(GPU processing - NVIDIA T4). \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rushvanth/AFDetection/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPIDS Setup on Colab\n",
    "\n",
    "Borrowed from [Rapids.ai](https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&offline=true&sandboxMode=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Sanity Check\n",
    "\n",
    "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
    "\n",
    "Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4, P4, or P100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup:\n",
    "Set up script installs\n",
    "1. Updates gcc in Colab\n",
    "1. Installs Conda\n",
    "1. Install RAPIDS' current stable version of its libraries, as well as some external libraries including:\n",
    "     1. cuDF\n",
    "     2. cuML\n",
    "     3. cuGraph\n",
    "     4. cuSpatial\n",
    "     5. cuSignal\n",
    "     6. BlazingSQL\n",
    "     7. xgboost\n",
    "1. Copy RAPIDS .so files into current working directory, a neccessary workaround for RAPIDS+Colab integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
    "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!python rapidsai-csp-utils/colab/env-check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will update the Colab environment and restart the kernel. Don't run the next cell until you see the session crash.\n",
    "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will install CondaColab. This will restart your kernel one last time. Run this cell by itself and only run the next cell once you see the session crash.\n",
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now run the rest of the cells as normal\n",
    "import condacolab\n",
    "condacolab.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'\n",
    "# The <release> options are 'stable' and 'nightly'.  Leaving it blank or adding any other words will default to stable.\n",
    "!python rapidsai-csp-utils/colab/install_rapids.py stable\n",
    "import os\n",
    "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
    "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
    "os.environ['CONDA_PREFIX'] = '/usr/local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "%pip install -r requirements.txt\n",
    "# Setup directory structures - create images/general and data\n",
    "!mkdir -p images/general\n",
    "!mkdir data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, make sure to upload the pre-processed data to the `data/` directory and the `requirements.txt` to the base direcoty. If you are cloning the entire repository and running this notebook, you can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate visualizations of classification results\n",
    "\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import  scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def svm_feature_importance(coef, names, fig_path, plot_title):\n",
    "    \"\"\"Visualize the feature importance of the SVM classifier.\"\"\"\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    # Plot the feature importance\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.title(f'{plot_title} Feature Importance')\n",
    "    plt.savefig(os.path.join(fig_path, 'feature_importance.png'))\n",
    "\n",
    "\n",
    "def visualize_results(results, classifier_name):\n",
    "    \"\"\"Visualize ROC Curve, Confusion Matrix, Classification Report and Feature Importance.\"\"\"\n",
    "    feature_names = results['x_train'].columns.tolist()\n",
    "    # Derive plot titles from classifier name. Make first letter uppercase and replace underscores with spaces\n",
    "    plot_title = classifier_name.title().replace('_', ' ')\n",
    "    # Join classifier_name with images_path\n",
    "    fig_path = os.path.join('images', classifier_name)\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(fig_path):\n",
    "        os.makedirs(fig_path)\n",
    "    # Feature Importance\n",
    "    # If Classifier name has svm, use a different method to visualize feature importance\n",
    "    if 'linear_svm' == classifier_name:\n",
    "        svm_feature_importance(results['model'].coef_[0], feature_names, fig_path, plot_title)\n",
    "    elif 'decision_tree' in classifier_name:\n",
    "        skplt.estimators.plot_feature_importances(results['model'], feature_names=results['x_train'].columns, figsize=(14,6))\n",
    "        plt.title(f'{plot_title} Feature Importance')\n",
    "        plt.savefig(os.path.join(fig_path, 'feature_importance.png'))\n",
    "    # ROC Curve\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(results['y_test'], results['y_pred'])\n",
    "    roc_curve.plot()\n",
    "    plt.title(f'{plot_title} ROC Curve')\n",
    "    plt.savefig(os.path.join(fig_path, 'roc_curve.png'))\n",
    "    # Confusion Matrix\n",
    "    skplt.metrics.plot_confusion_matrix(results['y_test'], results['y_pred'])\n",
    "    plt.title(f'{plot_title} Confusion Matrix')\n",
    "    plt.savefig(os.path.join(fig_path, 'confusion_matrix.png'))\n",
    "    # Plot Precision-Recall Curve\n",
    "    precision, recall, _ = metrics.precision_recall_curve(results['y_test'], results['y_pred'])\n",
    "    precision_recall_display = metrics.PrecisionRecallDisplay(precision, recall)\n",
    "    precision_recall_display.plot()\n",
    "    plt.title(f'{plot_title} Precision-Recall Curve')\n",
    "    plt.savefig(os.path.join(fig_path, 'precision_recall_curve.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate reports for each model\n",
    "\n",
    "def generate_report(clf, classifier_name):\n",
    "\n",
    "    print(f\"{classifier_name} Results\\n\" + \"-\" * 50 + \"\\n\")\n",
    "    print('Accuracy: {:.4F} \\n'.format(metrics.accuracy_score(clf['y_test'], clf['y_pred']) * 100))\n",
    "    print('Confusion Matrix: \\n', metrics.confusion_matrix(clf['y_test'], clf['y_pred']))\n",
    "    print('Area under curve: {:.4F} \\n'.format(metrics.roc_auc_score(clf['y_test'], clf['y_pred'])))\n",
    "    print(metrics.classification_report(clf['y_test'], clf['y_pred']))\n",
    "    print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate learning curves\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cuml.svm import SVC\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "def prepare_learning_curve_data(X, y, classifier_name, title, kernel=None):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(5, 20))\n",
    "    # title = r\"Learning Curves (SVM, Linear Kerner, $\\gamma=0.001$)\"\n",
    "    # SVC is more expensive so we do a lower number of CV iterations:\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "    if 'svm' in classifier_name:\n",
    "        if kernel:\n",
    "            estimator = SVC(kernel=kernel, gamma=0.001)\n",
    "        else:\n",
    "            estimator = SVC(gamma=0.001)\n",
    "    elif classifier_name == \"random_forest\":\n",
    "        estimator = RandomForestClassifier(n_estimators=100)    \n",
    "    elif classifier_name == \"knn\":\n",
    "        estimator = KNeighborsClassifier(n_neighbors=10)\n",
    "    learning_curve = plot_learning_curve(\n",
    "        estimator, title, X, y, axes=axes, ylim=(0.7, 1.01), cv=cv\n",
    "    )\n",
    "\n",
    "    plt.savefig(f'images/learning_curves/{classifier_name}_learning_curve.png')\n",
    "    return learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Curve functions\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "def random_forest_validation_curve(X, y):\n",
    "    rf = RandomForestClassifier()\n",
    "    param_range = np.arange(1,250,10)\n",
    "\n",
    "    train_scores, test_scores = validation_curve(rf, \n",
    "                                                X, \n",
    "                                                y, \n",
    "                                                param_name=\"n_estimators\", \n",
    "                                                param_range=param_range,\n",
    "                                                cv=3\n",
    "                                                )\n",
    "\n",
    "    # Calculate mean and standard deviation for training set scores\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "    # Calculate mean and standard deviation for test set scores\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot mean accuracy scores for training and test sets\n",
    "    plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
    "    plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
    "\n",
    "    # Plot accurancy bands for training and test sets\n",
    "    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n",
    "    plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n",
    "\n",
    "    # Create plot\n",
    "    plt.title(\"Validation Curve With Random Forest\")\n",
    "    plt.xlabel(\"Number Of Trees\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig('images/validation_curves/rf_validation_curve.png')\n",
    "\n",
    "def knn_validation_curve(X, y):\n",
    "    param_range = np.arange(1,20,1)\n",
    "    # Calculate accuracy on training and test set using the\n",
    "    # gamma parameter with 5-fold cross validation\n",
    "    train_score, test_score = validation_curve(KNeighborsClassifier(), X, y,\n",
    "                                        param_name = \"n_neighbors\",\n",
    "                                        param_range = param_range,\n",
    "                                            cv = 5)\n",
    "    \n",
    "    # Calculating mean and standard deviation of training score\n",
    "    mean_train_score = np.mean(train_score, axis = 1)\n",
    "    std_train_score = np.std(train_score, axis = 1)\n",
    "    \n",
    "    # Calculating mean and standard deviation of testing score\n",
    "    mean_test_score = np.mean(test_score, axis = 1)\n",
    "    std_test_score = np.std(test_score, axis = 1)\n",
    "    \n",
    "    # Plot mean accuracy scores for training and testing scores\n",
    "    plt.plot(param_range, mean_train_score,\n",
    "        label = \"Training Score\", color = 'b')\n",
    "    plt.plot(param_range, mean_test_score,\n",
    "    label = \"Cross Validation Score\", color = 'g')\n",
    "    \n",
    "    # Creating the plot\n",
    "    plt.title(\"Validation Curve with KNN Classifier\")\n",
    "    plt.xlabel(\"Number of Neighbours\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.savefig('images/validation_curves/knn_validation_curve.png')\n",
    "\n",
    "def svm_validation_curve(X, y, kernel=None):\n",
    "    param_range = np.logspace(-6, -1, 5)\n",
    "    if kernel:\n",
    "        svm = SVC(kernel=kernel)\n",
    "    else:\n",
    "        svm = SVC()\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        svm,\n",
    "        X,\n",
    "        y,\n",
    "        param_name=\"gamma\",\n",
    "        param_range=param_range,\n",
    "        scoring=\"accuracy\"\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(\"Validation Curve with SVM\")\n",
    "    plt.xlabel(r\"$\\gamma$\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    lw = 2\n",
    "    plt.semilogx(\n",
    "        param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        param_range,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.2,\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "    )\n",
    "    plt.semilogx(\n",
    "        param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        param_range,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.2,\n",
    "        color=\"navy\",\n",
    "        lw=lw,\n",
    "    )\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig('images/validation_curves/svm_validation_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Metadata for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Data and generate some metadata regarding the classes\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the file from the data directory\n",
    "af_data = pd.read_csv('data/Preprocessed_AFData.csv')\n",
    "# Print some metadata about the classes\n",
    "(unique,counts) = np.unique(af_data['Control'],return_counts=True)\n",
    "print(\"Metadata:\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(f\"Classes: {str(unique)} \\n\")\n",
    "print(\"Class Labels: \\n 0 - Non AF\\n 1 - AF\\n\")\n",
    "print(f\"Data in the 'Control' column: {dict(zip(unique,counts))} \\n\")\n",
    "print(f\"Ratio of occurrences of each class: {dict(zip(unique,counts/len(af_data['Control'])))}\\n\")\n",
    "print(\"-\"*50 + \"\\n\")\n",
    "# Display counts on a graph and save it\n",
    "target_variables = ['Non-AF','AF']\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.barplot(x=target_variables,y=counts)\n",
    "plt.title('Count of AF and Non-AF occurrences in the Preprocessed data')\n",
    "plt.ylabel('Count')\n",
    "for i,_ in enumerate(counts):\n",
    "    plt.text(i-0.25, counts[i]+0.5, counts[i], color='black', fontweight='bold')\n",
    "img_file_path = 'images/general/count_of_AF_and_Non_AF_occurrences.png'\n",
    "plt.savefig(img_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Correlation Between Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of the correlation matrix\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(af_data.corr(), vmin=-1, vmax=1, interpolation='none')\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(af_data.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(af_data.columns)\n",
    "ax.set_yticklabels(af_data.columns)\n",
    "plt.title('Correlation Matrix of the Preprocessed data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Density Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features densities depending on the outcome values\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 20\n",
    "\n",
    "# separate data based on outcome values \n",
    "outcome_0 = af_data[af_data['Control'] == 0]\n",
    "outcome_1 = af_data[af_data['Control'] == 1]\n",
    "names = list(af_data.columns)\n",
    "# plot densities for outcomes\n",
    "i=0\n",
    "j=0\n",
    "for _ in range(3):\n",
    "    # init figure\n",
    "    fig, axs = plt.subplots(10, 1)\n",
    "    fig.suptitle('Features densities for different outcomes 0/1')\n",
    "    plt.subplots_adjust(left = 0.25, right = 0.9, bottom = 0.1, top = 0.95,\n",
    "                        wspace = 0.2, hspace = 0.9)\n",
    "    for column_name in names[i:i+10]: \n",
    "        ax = axs[j]\n",
    "        #plt.subplot(4, 2, names.index(column_name) + 1)\n",
    "        outcome_0[column_name].plot(kind='density', ax=ax, subplots=True, \n",
    "                                    sharex=False, color=\"red\", legend=True,\n",
    "                                    label=column_name + ' for Outcome = 0')\n",
    "        outcome_1[column_name].plot(kind='density', ax=ax, subplots=True, \n",
    "                                        sharex=False, color=\"green\", legend=True,\n",
    "                                        label=column_name + ' for Outcome = 1')\n",
    "        ax.grid('on')\n",
    "        j+=1\n",
    "    i+=10\n",
    "    j=0\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data to obtain Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = af_data.drop(['Control'],axis=1)\n",
    "y = af_data['Control']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM\n",
    "plot = prepare_learning_curve_data(X, y, 'linear_svm', r'Learning Curve (SVM, Linear Kernel, $\\gamma=0.001)', kernel='linear')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "\n",
    "plot = prepare_learning_curve_data(X, y, classifier_name='rbf_svm', title= r'Learning Curve (SVM, RBF Kernel, $\\gamma=0.001)')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "# Convert data to float32 as cuML's RF implementation only accepts float32 data\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "plot = prepare_learning_curve_data(X, y, classifier_name='random_forest', title=r'Learning Curve (Random Forest)')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier learning curves\n",
    "plot = prepare_learning_curve_data(X, y, classifier_name='knn', title=r'Learning Curve (10-Nearest Neighbours)')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM Validation Curve\n",
    "validation_plot = svm_validation_curve(X, y, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel Validation Curve\n",
    "validation_plot = svm_validation_curve(X, y, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Validation Curve\n",
    "validation_plot = random_forest_validation_curve(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Validation Curve\n",
    "validation_plot = knn_validation_curve(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ML Classifiers to predict AF outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM\n",
    "kernel = 'linear'\n",
    "print(f\"Training Support Vector Machine with {kernel} kernel...\\n\")\n",
    "svm = SVC(kernel=kernel, random_state=101)\n",
    "svm.fit(x_train, y_train)\n",
    "y_pred = svm.predict(x_test)\n",
    "clf = {'model': svm, 'x_train': x_train, 'x_test': x_test, 'y_train': y_train, 'y_test': y_test, 'y_pred': y_pred}\n",
    "generate_report(clf, 'linear_svm')\n",
    "visualize_results(clf, 'linear_svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF kernel\n",
    "kernel = 'rbf'\n",
    "print(f\"Training Support Vector Machine with {kernel} kernel...\\n\")\n",
    "svm = SVC(kernel=kernel, random_state=101)\n",
    "svm.fit(x_train, y_train)\n",
    "y_pred = svm.predict(x_test)\n",
    "clf = {'model': svm, 'x_train': x_train, 'x_test': x_test, 'y_train': y_train, 'y_test': y_test, 'y_pred': y_pred}\n",
    "generate_report(clf, 'linear_svm')\n",
    "visualize_results(clf, 'rbf_svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print(f\"Training Decision Tree...\\n\")\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=101)\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "clf = {'model': dt, 'x_train': x_train, 'x_test': x_test, 'y_train': y_train, 'y_test': y_test, 'y_pred': y_pred}\n",
    "generate_report(clf, 'decision_tree')\n",
    "visualize_results(clf, 'decision_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(f\"Training Random Forest...\\n\")\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=101)\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "clf = {'model': rf, 'x_train': x_train, 'x_test': x_test, 'y_train': y_train, 'y_test': y_test, 'y_pred': y_pred}\n",
    "generate_report(clf, 'random_forest')\n",
    "visualize_results(clf, 'random_forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Class Imbalance\n",
    "\n",
    "1. Weighted ML models\n",
    "2. SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Weighted Logistic Regression and Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg = LogisticRegression(class_weight=None, max_iter=1000)\n",
    "lg.fit(x_train, y_train)\n",
    "y_pred = lg.predict(x_test)\n",
    "clf = {'model': lg, 'x_train': x_train, 'x_test': x_test, 'y_train': y_train, 'y_test': y_test, 'y_pred': y_pred}\n",
    "generate_report(clf, 'logistic_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defind class weight for Logistic Regression\n",
    "class_weight = {0: 25, 1: 75}\n",
    "weighted_lg = LogisticRegression(class_weight=class_weight, max_iter=1000)\n",
    "weighted_lg.fit(x_train, y_train)\n",
    "y_pred = weighted_lg.predict(x_test)\n",
    "clf = {'model': weighted_lg, 'x_train': x_train, 'x_test': x_test, 'y_train': y_train, 'y_test': y_test, 'y_pred': y_pred}\n",
    "generate_report(clf, 'weighted_logistic_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Grid search to find the best weights for the Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from cuml.model_selection import GridSearchCV\n",
    "\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "#Creating a dictionary grid for grid search\n",
    "hyperparam_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "weighted_lg_with_gridsearch = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(estimator=weighted_lg_with_gridsearch, param_grid=hyperparam_grid, scoring='roc_auc', refit=True)\n",
    "grid_search.fit(x_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best parameters: {best_parameters}\")\n",
    "print(f\"Best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameters: {'class_weight': {0: 0.06964824120603015, 1: 0.9303517587939698}}\n",
    "- Best score: 0.9653332483643895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploting the score for different values of weight\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(12,8))\n",
    "weigh_data = pd.DataFrame({ 'score': grid_search.cv_results_['mean_test_score'], 'weight': (1- weights)})\n",
    "sns.lineplot(weigh_data['weight'], weigh_data['score'])\n",
    "plt.xlabel('Weight for class 1')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.xticks([round(i/10,1) for i in range(0,11,1)])\n",
    "plt.title('Scoring for different class weights', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted SVM using best parameters found in Logistic Regression Grid Search\n",
    "print(f\"Training Weighted SVM...\\n\")\n",
    "weighted_svm = SVC(kernel='linear', class_weight=best_parameters['class_weight'], random_state=101)\n",
    "weighted_svm.fit(x_train, y_train)\n",
    "y_pred = weighted_svm.predict(x_test)\n",
    "clf = {'model': weighted_svm, 'x_train': x_train, 'x_test': x_test, 'y_train': y_train, 'y_test': y_test, 'y_pred': y_pred}\n",
    "generate_report(clf, 'weighted_svm')\n",
    "visualize_results(clf, 'weighted_svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMOTE to perform oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_oversampled, y_oversampled = oversample.fit_resample(X,y)\n",
    "unique, counts = np.unique(y_oversampled, return_counts=True)\n",
    "print(f\"Data in Control class after oversampling: {dict(zip(unique, counts))} \\n\")\n",
    "target_variables = ['Non-AF','AF']\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.barplot(x=target_variables,y=counts)\n",
    "plt.title('Count of AF and Non-AF occurrences in the Preprocessed data')\n",
    "plt.ylabel('Count')\n",
    "for i,_ in enumerate(counts):\n",
    "    plt.text(i-0.25, counts[i]+0.5, counts[i], color='black', fontweight='bold')\n",
    "img_file_path = 'images/general/count_of_AF_and_Non_AF_occurrences.png'\n",
    "plt.savefig(img_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine SMOTE with random undersampling of the majority class\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "oversample = SMOTE(sampling_strategy=0.33)\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('oversample', oversample), ('undersample', undersample)]\n",
    "pipeline = Pipeline(steps)\n",
    "X_oversampled, y_oversampled = pipeline.fit_resample(X,y)\n",
    "unique, counts = np.unique(y_oversampled, return_counts=True)\n",
    "print(f\"Data in Control class after SMOTE + random undersampling of majority class: {dict(zip(unique, counts))} \\n\")\n",
    "target_variables = ['Non-AF','AF']\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.barplot(x=target_variables,y=counts)\n",
    "plt.title('Count of AF and Non-AF occurrences in the Preprocessed data')\n",
    "plt.ylabel('Count')\n",
    "for i,_ in enumerate(counts):\n",
    "    plt.text(i-0.25, counts[i]+0.5, counts[i], color='black', fontweight='bold')\n",
    "img_file_path = 'images/general/count_of_AF_and_Non_AF_occurrences_after_SMOTE_and_random_undersampling.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking - Metrics and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def run_exps(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Lightweight script to test many models and find winners\n",
    ":param X_train: training split\n",
    "    :param y_train: training target vector\n",
    "    :param X_test: test split\n",
    "    :param y_test: test target vector\n",
    "    :return: DataFrame of predictions\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "    models = [\n",
    "              (\"DecisionTree\", DecisionTreeClassifier(max_depth=5)),\n",
    "              ('LogReg', LogisticRegression(max_iter=1000)), \n",
    "              ('RF', RandomForestClassifier()),\n",
    "              ('KNN', KNeighborsClassifier()),\n",
    "              ('SVM', SVC()),\n",
    "              ('Linear_SVM', SVC(kernel='linear'))\n",
    "            ]\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "    target_names = ['Non-AF', 'AF']\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(name)\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        this_df = pd.DataFrame(cv_results)\n",
    "        this_df['model'] = name\n",
    "        dfs.append(this_df)\n",
    "    final = pd.concat(dfs, ignore_index=True)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstraps = []\n",
    "for model in list(set(final.model.values)):\n",
    "    model_df = final.loc[final.model == model]\n",
    "    bootstrap = model_df.sample(n=30, replace=True)\n",
    "    bootstraps.append(bootstrap)\n",
    "        \n",
    "bootstrap_df = pd.concat(bootstraps, ignore_index=True)\n",
    "results_long = pd.melt(bootstrap_df,id_vars=['model'],var_name='metrics', value_name='values')\n",
    "time_metrics = ['fit_time','score_time'] # fit time metrics\n",
    "## PERFORMANCE METRICS\n",
    "results_long_nofit = results_long.loc[~results_long['metrics'].isin(time_metrics)] # get df without fit data\n",
    "results_long_nofit = results_long_nofit.sort_values(by='values')\n",
    "## TIME METRICS\n",
    "results_long_fit = results_long.loc[results_long['metrics'].isin(time_metrics)] # df with fit data\n",
    "results_long_fit = results_long_fit.sort_values(by='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.set(font_scale=2.5)\n",
    "g = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_nofit, palette=\"Set3\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Comparison of Model by Classification Metric')\n",
    "plt.savefig('images/general/benchmark_models_performance.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "sns.set(font_scale=2.5)\n",
    "g = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_fit, palette=\"Set3\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Comparison of Model by Fit and Score Time')\n",
    "plt.savefig('images/general/benchmark_models_time.png',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83bb4f012846a47f4d52528631dd63e6cfba4dd4be145200a10778e885aa240e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
